{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f74d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1228 entries, 0 to 1227\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   Title                 1228 non-null   object\n",
      " 1   Year                  1228 non-null   int64 \n",
      " 2   Authors               1228 non-null   object\n",
      " 3   Label                 1228 non-null   object\n",
      " 4   Partitioned Abstract  1228 non-null   object\n",
      " 5   Target                1228 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 57.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame to be used in the clustering process\n",
    "df = pd.read_csv('Data/Partitioned_Abstracts.csv', sep=',', header=0, encoding='utf-8')\n",
    "\n",
    "#verify that the DataFrame is loaded correctly\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4acddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bce12ee2c04373ad363060f257d5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1228, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Create the combined text by concatenating the \"Title\" and \"Partitioned Abstract\" columns\n",
    "combined_texts = df[\"Title\"].astype(str) + \"\\n\\n\" + df[\"Partitioned Abstract\"].astype(str)\n",
    "# Convert the \"Target\" column to integer type for classification purposes\n",
    "targets = df[\"Target\"].astype(int)\n",
    "\n",
    "# Load the SentenceTransformer model for generating embeddings\n",
    "model = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "# Define the task type for the model\n",
    "task = \"clustering\"\n",
    "\n",
    "# Encode the combined texts using the SentenceTransformer model\n",
    "embeddings = model.encode(\n",
    "    combined_texts.tolist(),\n",
    "    show_progress_bar=True,\n",
    "    device=\"cuda\",\n",
    "    convert_to_tensor=True,\n",
    ")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evoc\n",
    "import torch\n",
    "\n",
    "# Initialize the EVoC clustering algorithm\n",
    "clusterer = evoc.EVoC(\n",
    "    #min_num_clusters=10\n",
    "    n_neighbors = 60\n",
    ")\n",
    "# Fit the EVoC clustering algorithm to the embeddings and predict the cluster labels\n",
    "labels = clusterer.fit_predict(embeddings.cpu().to(dtype=torch.float32).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3368b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as 'evoc_clusters_plot.html'\n"
     ]
    }
   ],
   "source": [
    "import evoc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Run UMAP to reduce the dimensionality of the embeddings to 3D for visualization\n",
    "umap_3d = umap.UMAP(\n",
    "    n_components=3,\n",
    "    random_state=42,\n",
    "    n_neighbors=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine'\n",
    ").fit_transform(embeddings.cpu().to(dtype=torch.float32).numpy())\n",
    "\n",
    "# Prepare the DataFrame for plotting\n",
    "df_plot = pd.DataFrame(umap_3d, columns=[\"x\", \"y\", \"z\"])\n",
    "df_plot[\"Cluster\"] = labels\n",
    "df_plot[\"Title\"] = df[\"Title\"]\n",
    "df_plot[\"FullText\"] = combined_texts  # List of concatenated title+abstract, or similar\n",
    "\n",
    "# Generate topic labels with BERTopic\n",
    "topic_labels = {}\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=None,\n",
    "    lowercase=True,\n",
    "    max_features=3000\n",
    ")\n",
    "cluster_texts = df_plot[df_plot[\"Cluster\"] != -1].groupby(\"Cluster\")[\"FullText\"].apply(list)\n",
    "\n",
    "\n",
    "for label, texts in cluster_texts.items():\n",
    "    try:\n",
    "        topic_model = BERTopic(\n",
    "            vectorizer_model=vectorizer,\n",
    "            calculate_probabilities=False,\n",
    "            # representation_model=KeyBERTInspired(),\n",
    "            verbose=False\n",
    "        )\n",
    "        topics, _ = topic_model.fit_transform(texts)\n",
    "        \n",
    "        \n",
    "\n",
    "        # FIXED: convert dict_keys to list before indexing\n",
    "        top_topic_id = list(topic_model.get_topics().keys())[0]\n",
    "        top_words = topic_model.get_topic(top_topic_id)\n",
    "\n",
    "        label_text = \", \".join([word for word, _ in top_words[:3]])\n",
    "        topic_labels[label] = label_text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cluster {label} failed: {e}\")\n",
    "        topic_labels[label] = f\"Cluster {label}\"\n",
    "\n",
    "# ðŸ”§ Assign ClusterLabel using the topic labels\n",
    "df_plot[\"ClusterLabel\"] = df_plot[\"Cluster\"].apply(\n",
    "    lambda x: \"Noise\" if x == -1 else topic_labels.get(x, f\"Cluster {x}\")\n",
    ")\n",
    "\n",
    "\n",
    "# ðŸŽ¨ Create a color map\n",
    "unique_labels = df_plot[\"ClusterLabel\"].unique()\n",
    "base_colors = px.colors.qualitative.Dark2\n",
    "color_map = {}\n",
    "color_index = 0\n",
    "\n",
    "for label in unique_labels:\n",
    "    if label == \"Noise\":\n",
    "        color_map[label] = \"darkgrey\"\n",
    "    else:\n",
    "        color_map[label] = base_colors[color_index % len(base_colors)]\n",
    "        color_index += 1\n",
    "\n",
    "# Sort so noise is plotted last\n",
    "df_plot = df_plot.sort_values(by=\"Cluster\", key=lambda col: col == -1)\n",
    "\n",
    "# ðŸ“Š Create interactive 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    df_plot,\n",
    "    x=\"x\", y=\"y\", z=\"z\",\n",
    "    color=\"ClusterLabel\",\n",
    "    color_discrete_map=color_map,\n",
    "    hover_name=\"Title\",\n",
    "    title=\"EVoC Clusters in 3D\",\n",
    "    opacity=0.8\n",
    ")\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title=\"X\",\n",
    "    yaxis_title=\"Y\",\n",
    "    zaxis_title=\"Z\",\n",
    "    bgcolor=\"lightgrey\"\n",
    "))\n",
    "\n",
    "\n",
    "fig.write_html(\"evoc_clusters_plot.html\")\n",
    "print(\"Plot saved as 'evoc_clusters_plot.html'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68c1544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohenâ€™s Kappa (EVoC vs. Target): 0.7922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "# Filter to remove noise\n",
    "mask = labels != -1\n",
    "\n",
    "# Ground truth\n",
    "true_labels = df[\"Target\"].values  # shape: (1228,)\n",
    "\n",
    "cluster_labels = labels[mask]\n",
    "gt_labels = true_labels[mask]\n",
    "\n",
    "assert len(cluster_labels) == len(gt_labels)\n",
    "\n",
    "# Hungarian matching to align clusters with ground truth\n",
    "def align_clusters(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "    mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
    "    aligned = np.array([mapping.get(p, -1) for p in y_pred])\n",
    "    return aligned\n",
    "\n",
    "aligned_preds = align_clusters(gt_labels, cluster_labels)\n",
    "\n",
    "# Compute Cohen's Kappa\n",
    "kappa = cohen_kappa_score(gt_labels, aligned_preds)\n",
    "print(f\"Cohenâ€™s Kappa (EVoC vs. Target): {kappa:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
